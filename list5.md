1. #### Train [Whisper](https://github.com/openai/whisper) on EEG spectrogram and text(or label) pair datasets(like [MindBigData "IMAGENET"]([http://www.mindbigdata.com/](http://www.mindbigdata.com/opendb/imagenet.html))), to turn EEG spectrograms into text(or labels) (thoughts to text). Use the guide from https://huggingface.co/blog/fine-tune-whisper to train Whisper. The model would be used to decode EEG spectrograms into text(or labels).
2. #### Make an Gradio app(or notebook) that would turn text(description) into a 3D mesh/model using [Stable Diffusion](https://github.com/CompVis/stable-diffusion) to turn text into a image and [Dense Prediction Transformer (DPT)](https://huggingface.co/Intel/dpt-large) to compute a depth map from the image and then use [open3d](www.open3d.org) to compute a 3D mesh(object) from the depth map and the image. The image to 3D object using DPT(and open3d) idea is from https://huggingface.co/spaces/radames/dpt-depth-estimation-3d-obj, the only difference would be to use stable diffusion to generate the input image. 
3. #### Make an 3D RPG(in Godot or Unreal Engine 5) that that would generate 3D assets on the fly (continuously) using [DreamFusion](https://github.com/ashawkey/stable-dreamfusion). The map would be dynamic and populated on the fly with the generated 3D assets, the 3D assets could be generated instanlty on the fly or in advance on the client or server. The map creation would work like this: first a 2D or 3D map would be created by an algorithm like [WaveFunctionCollapse](https://github.com/mxgmn/WaveFunctionCollapse) or some other algorithm. The algorithm would place object(like bridges, lanterns, fruits, people/npc's, etc) randomly or using the WaveFunctionCollapse either the on the fly generated 3D assets or it would place placeholder models that would be later replaced with the generated 3D models.

